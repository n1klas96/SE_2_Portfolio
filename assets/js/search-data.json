{
  
    
        "post0": {
            "title": "Diagramme",
            "content": "Diagramme . . Requirements . . Datenfluss . . Data Science Architektur . .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/07diagramme.html",
            "relUrl": "/markdown/2022/06/24/07diagramme.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Design und Data Science Architektur",
            "content": "Design und Data Science Architektur . . Design (Refactoring) . Um den Code übersichtlicher zu machen und zu verbessern, wurde innerhalb des Projekts ein Refactoring-Prozess verwendet. Der Code wurde dabei in Funktionen und Dateien aufgeteilt. Das Resultat des Refactoring ist, dass der Code einerseits besser testbar und andererseits konfigurabel gemacht wurde. . Data Science Architektur . Dashboard | . Das Dashboard von Frunch Infinity wurde mittels Streamlit entwickelt. Die Bedienung des Dashboards ist sehr einfach und verzichtet auf unnötige Funktionen. In einer Sidebar ist der zu trainierende Datensatz auswählbar (derzeit lediglich Turbofan), ob es sich um ein Regressions -oder Klassifikations-task handelt (vorerst nur Regression), ein Button zum Anzeigen der letzten Trainingsergebnisse und ein weiterer Button zum Starten des Re-Training’s. Das Dashboard weist also durch seine minimalistischen Funktionen eine gewisse User-Freundlichkeit auf und die Möglichkeit die trainierten Modelle direkt und übersichtlich zu evaluieren. Der größte Nachteil des Dashboards ist derzeit, dass das Modell-Training mit PyCaret sehr lange dauert (ca. 8min). Dies ist natürlich schlecht für die User-Experience und daher muss die Performance innerhalb des Model-Monitoring implementiert werden. . Pipeline Ansatz | . Derzeit verwenden wir im Projekt ledigleich eine sKlearn Pipeline, die die Daten für das anschließende Modell-Training aufbereitet. Möglich wäre auch die Implementierung einer Apache Airflow Pipeline um verschiedene Aufgaben innerhalb des Systems zu parallelisieren und dadurch die Performance zu steigern. . CI/CD Pipeline | . Des weiteren haben wir eine Pipeline für CI/CD (.gitlab-ci.yaml) aufgesetzt, die zwei verschiendene Stages hat. In der test Stage haben wir zum einen den test-job implementiert, welcher die Unit-tests durchführt. Im Pages-job, welcher parallel läuft, wird neben den dependencies auch die Dokumentation mittels PDOC erstellt. Innerhalb der build stage wird ein Image vom Docker-File erstellt, dieses wird auf GitLab gespeichert und bei einem Release würde dann (wenn es eine deploy stage gäbe) die Applikation in einem docker container aufgebaut. . Model Performance | . Wird das ML-Modell in der Streamlit-Applikation zum Re-Training getriggert, erscheint auf der UI ein Spinner, welcher eine sogenannte queue für ML-Jobs bereitstellt. Sobald das Training beendet ist, bekommt der User eine Benachrichtigung, dass die Trainingsresultate nun via Klick auf den “Letzte Trainingsergebnisse anzeigen” Button verfügbar sind. Dies verhindert zwar nicht die lange Wartezeit, verhindert allerdings, das der User die Seite verlässt und vermutet das die Applikation Fehler aufweist. . Zu der Data Science Architektur des Projekts kann abschließend gesagt werden, dass der CRISP-DM Zyklus in Verbindung mit kontinuierlicher Evaluation, Retraining und Monitoring recht gut passt. Allerdings muss das Segment des Monitoring deutlich stärker eingebunden werden um beispielsweise die Performance des Retrainings zu senken. . .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/06design.html",
            "relUrl": "/markdown/2022/06/24/06design.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Architektur",
            "content": "Projekt-Architektur . . Monolitische Architektur . Innerhalb des Projekts Frunch Infinity wird größtenteils eine monolytische Architektur verwendet. Prinzipiell ist das System mit diversen Komponenten, wie beispielsweise der processing.py, model.py und ui.py miteinander verbunden, voneinander abhängig und nicht, wie in modularen Systemen, lose gekoppelt. Der Vorteil modularer Systeme im Gegensatz zu einem Monolithen ist, dass einzelne Module losgelöst voneinander verändert werden können. Dies ist in unserem System nicht vollumfänglich möglich. Einige Komponenten wie beispielsweise model.py, erwarten eine bestimmte Vorverarbeitung der Pipeline aus pipeline.py. Werden hier grundlegende Änderungen vorgenomme, müsste model.py ebenfalls angepasst werden. Trotzdem hat auch die monolitische Architektur wichtige Vorteil gegenüber einem modularen Aufbau bzw. Microservices. Durch eine monolitische Architektur ist es möglich die Software leichter zu testen, als auch zu debuggen. . Dokumentation des Systems . Die Dokumentation wurde mithilfe von Pdoc erstellt. Dabei werden sämtliche Docstring, welche in den verschiedenen Skripten verwendet wurden, zu einer HTML-Datei zusammengefügt und über GitLab Pages bereitgestellt. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/05architektur.html",
            "relUrl": "/markdown/2022/06/24/05architektur.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Web - Technologien",
            "content": "Web-Technologien von Frunch Infinity . . Streamlit . Die Ergebnisse des Projekts bzw. des Auto-ML-Prozesses werden innerhalb einer Streamlit Applikation visualisiert. Diese wird wiederrum von Streamlit Cloud gehostet und ist somit öffentlich zugänglich. Durch Streamlit ist es einerseits sehr schnell möglich Ergebnisse anschaulich dazustellen, aber auch sie in einem relativ leichten Prozess durch GitHub und Streamlit Cloud zu deployen. . SQL-Alchemy (Data Warehouse) . Durch SQL-Alchemy stellen wir unserem System ein Data Warehouse bereit, in dem einerseits die Trainingsergebnisse unseres Modells abgespeichert und dementsprechend auch abgefragt werden können. Zudem wird das Target Feature, welches in der Streamlit-Applikation auswählbar ist, ebenfalls im Data Warehouse abgelegt. . PyCaret . Durch die Hilfe von PyCaret werden insgesamt 17 verschiedene Modelle trainiert und im Anschluss mit Hilfe verschiedener Metriken verglichen. Der ausgegebene Data Frame wir nach Beendigung des Trainings in der Streamlit App dargestellt . Poetry . Innerhalb der Projektorganisation nutzen das Package Management System von Poetry. Innerhalb des poetry.lock files werden sämtliche Paket Requirements gelistet. Durch Poetry arbeiten alle Entwickler auf den gleichen Versionsebenen der verschiedenen Pakete. . Hydra . Als Pfad-Management-Tool verwenden wir das Framework von Hydra. Durch Hydra ist es möglich beispielsweise den Daten-Pfad einmal zentral in config (main.yaml) festzulegen und diesen Ort durch den decorator von hydra überall anzusprechen. Sollte sich der Ort irgendwann ändern, kann dies zentral an einem Ort erfolgen . Cookiecutter . Für die Organisation und die Strukturierung des Projekt Repositorys haben ein Template von Cookiecutter verwendet. Auch wenn nicht alle Vorlagen des Templates verwendet wurden, hat dies bei der Übersichtlichkeit des Projekts geholfen . Comit Control . Durch die .pre-commit-config-yaml werden in einige Richtlinien für den Abschluss eines Commits festgelegt. Ein Beispiel für eine solche Richtlinie ist beispielsweise isort zu nennen. Hierbei werden die Imports automatisch in sinnvoller Reihenfolge und alphabetisch sortiert. . Docker Container . Docker Container werden innerhalb des Projektes einerseits für dev-dependencies, die Entwicklung in virtuellen Umgebungen und den Aufbau des Frontends verwendet . Logging . In einem .log file werden sämtliche Prozesse die in der Ausgabe beispielsweise beim Modell-Training oder im Preprocessing mitlaufen abgespeichert. Dies ist insbesondere für Debugging essentiell. . Unittest . Es gibt verschiedene Unit-Tests im Projekt. Das Framework testet dabei unter anderem innerhalb der Preprocessing-Pipeline auf die korrekte shape der Daten, ob die entsprechenden Variablen entfernt wurden und der Pfad stimmt. Zudem gibt es einen Test, ob die Streamlit-Applikation aufrufbar ist. Durch die Unit- tests wird das gesamte System robuster und vereinfacht darüber hinaus das Debugging. . Pdoc . Durch die Software von Pdoc ist es innerhalb des Projekts möglich, sämtliche Docstrings in einer html-Datei zusammenzufassen und dementsprechend wichtige Informationen zu dokumentieren. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/04web-technologien.html",
            "relUrl": "/markdown/2022/06/24/04web-technologien.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Data Science Methodik-Konzept - Beschreibung",
            "content": ". Abbildung des Datenflusses . . . Business Understanding . Das Ziel des Projektes ist die Entwicklung eines Auto Machine Learning Prozesses welcher innerhalb einer UI Schnittstelle ausgelöst werden kann. Ausgehend von diesem Ziel müssen die Rohdaten des Turbofan-Datensatzes innerhalb einer Pipeline für das anschließende Training mit Pycaret aufbereitet werden. Sobald das Training der Modelle beendet ist, muss der Output (Metriken) der Modelle übersichtlich in einem Dataframe dargestellt werden. Innerhalb einer Streamlit-Applikation muss es möglich sein, jenen Dataframe auszugeben, sowie ein “Retraining” der Pipeline erneut zu initiieren. . Data Understanding . Die Datenbasis bildet der Turbofan-Datensatz, welcher in mehreren txt Dateien in data/raw des Projekt-Repositorys abliegt. Die Datensätze bestehen dabei aus mehreren multivariaten Zeitreihen. Jeder Datensatz wird zudem in Trainings- und Test-Teilsätze unterteilt. Außerdem stammt jede Zeitreihe aus einer unterschiedlichen Maschine. Die Textdateien werden mit 26 numerischen Variablen, getrennt durch Leerzeichen, bereitgestellt. . Data Preparation . In der Datenaufbereitung wird der Trainingsdatensatz zunächst für die “Preprocessing Pipeline” aufbereitet. Wichtige Schritte sind hierbei die Bestimmung der Input Feature, sowie des Target Features. Während das Taret Feature die Sensor-Werte 1-23 annehmen kann, müssen die Variablen: Unit-Number, Cycles und Operational Settings 1-3 in kategoriale Variablen konvertiert werden. Im Anschluss werden mithilfe einer Pipeline, fehlende und None Werte entfernt. zudem werden die numerischen Variablen skaliert. . Modeling . Durch die PyCaret Funktion .setup werden die numerischen, kategorialen Variablen, sowie das Target Feature für das anschließende Training initialisiert. Das eigentliche Training erfolgt durch die Funktion .compare_models() . Evaluierung . Nachdem das Modell-Training beendet ist, wird durch die .pull() Funktion ein Data Frame erzeugt, welcher die Trainingsresultate zusammenfasst. Der Data Frame beinhaltet eine Auflistung der unterschiedlichen Modelle, dem entsprechenden MAE, MSE, RMSE, R^2, RMSLE, MAPE und die verstrichene Trainingstzeit. Anhand dieser Metriken kann jedes Modell evaluiert werden . Deployment . Die Applikation sollte eigentlich mit Streamlit Cloud deployed werden, da mithilfe dieses Dienstes der Veröffentlichungsprozess recht einfach umsetzbar ist. Um die Applikation public zu machen, wird ledigleich ein GitHub-Repository mit den selben Inhalten wie in GitLab Repository benötigt. Im Anschluss kann direkt über die lokale Streamlit-App die build-in Funktion “deploy app” verwendet werden. Nachdem man dort den entsprechenden GitHub-Account und Repository angegeben hat, wird die Applikation automatisch deployed. Hierbei kann es zu Problemen kommen, wenn die App die von Streamlit breitgestellten Ressourcen übertrifft. Dieses Problem ist leider in unserem Fall aufgetreten, da die Modellierung mit PyCaret extrem viele Ressourcen braucht. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/02methodik-konzept.html",
            "relUrl": "/markdown/2022/06/24/02methodik-konzept.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Modellierte Anforderungen (Requirements)",
            "content": "Modellierte Requirements von Frunch Infinity . . . Veränderung der Requirements im Projektverlauf . In Stage 1 unseres Projektes haben wir die Requirements nach den Sophisten Regeln grundlegend festgelegt und auf Basis jener wurden die ersten, erforderlichen Entwicklungsschritte durchgeführt: . erstellung eines dev-containers | Aufbau einer preprocessing Pipeline | Aufbau einer CI/CD Pipeline | Unit-Tests für Pipelines | erste Version der Streamlit Applikation | Modellentwicklung mit PyCaret | . Nachdem die ersten Entwicklungsschritte von Stage 1 abgeschlossen waren, haben wir in unserem Sprint-Review einen neuen Requirement-Zyklus eingeleitet und die einzelnen Requirements neu priorisiert. Zunächst hat sich gezeigt das das Requirement: “Wenn der Turbofan-Datensatz in die Streamlit-Applikation importiert wurde, muss die Applikation alle Variablen und Werte des Datensatzes erkennen” nach unseren ersten Entwicklungserkenntnissen nicht ganz passend für unsere MVP-Software einer Auto-ML ist. Der Import von einem oder verschiedenen Datensätzen erschwert die Entwicklung eines ersten, funktionierenden Produkts enorm. Die Wichtigkeit unserer Requirements wurde also in diesem Zyklus angepasst auf zunächst einen Datensatz (Turbofan) und Regressions-Algorithmen priorisiert. . Nach jedem größeren Entwicklungsschritt bzw. in jedem Sprint-Review sollten die aufgestellten Requirements auf konsistenz und Vollständigkeit geprüft werden, da auf Veränderungen innerhalb der Requirements nur angemessen reagiert werden kann, wenn diese bekannt sind. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/01Requirements-Engineering.html",
            "relUrl": "/markdown/2022/06/24/01Requirements-Engineering.html",
            "date": " • Jun 24, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Projektteilnehmer: . Dalibor Mitic Fabian Kainz Niklas Mayr .",
          "url": "https://n1klas96.github.io/SE_2_Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://n1klas96.github.io/SE_2_Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}