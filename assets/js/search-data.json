{
  
    
        "post0": {
            "title": "Design und Data Science Architektur",
            "content": "Modellierte Requirements von Frunch Infinity . . . Veränderung der Requirements im Projektverlauf . In Stage 1 unseres Projektes haben wir die Requirements nach den Sophisten Regeln grundlegend festgelegt und auf Basis jener wurden die ersten, erforderlichen Entwicklungsschritte durchgeführt: . erstellung eines dev-containers | Aufbau einer preprocessing Pipeline | Aufbau einer CI/CD Pipeline | Unit-Tests für Pipelines | erste Version der Streamlit Applikation | Modellentwicklung mit PyCaret | . Nachdem die ersten Entwicklungsschritte von Stage 1 abgeschlossen waren, haben wir in unserem Sprint-Review einen neuen Requirement-Zyklus eingeleitet und die einzelnen Requirements neu priorisiert. Zunächst hat sich gezeigt das das Requirement: “Wenn der Turbofan-Datensatz in die Streamlit-Applikation importiert wurde, muss die Applikation alle Variablen und Werte des Datensatzes erkennen” nach unseren ersten Entwicklungserkenntnissen nicht ganz passend für unsere MVP-Software einer Auto-ML ist. Der Import von einem oder verschiedenen Datensätzen erschwert die Entwicklung eines ersten, funktionierenden Produkts enorm. Die Wichtigkeit unserer Requirements wurde also in diesem Zyklus angepasst auf zunächst einen Datensatz (Turbofan) und Regressions-Algorithmen priorisiert. . Nach jedem größeren Entwicklungsschritt bzw. in jedem Sprint-Review sollten die aufgestellten Requirements auf konsistenz und Vollständigkeit geprüft werden, da auf Veränderungen innerhalb der Requirements nur angemessen reagiert werden kann, wenn diese bekannt sind. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/06design.html",
            "relUrl": "/markdown/2022/06/24/06design.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Architektur",
            "content": "Projekt-Architektur . . Monolitische Architekur . Innerhalb des Projekts Frunch Infinity wird größtenteils eine monolytische Architektur verwendet. Prinzipiell ist das System mit diversen Komponenten, wie beispielsweise der processing.py, model.py und ui.py miteinander verbunden, voneinander abhängig und nicht, wie in modularen Systemen, lose gekoppelt. Der Vorteil modularer Systeme im Gegensatz zu einem Monolithen ist, dass einzelne Module losgelöst voneinander verändert werden können. Dies ist in unserem System nicht vollumfänglich möglich. Einige Komponenten wie beispielsweise model.py, erwarten eine bestimmte Vorverarbeitung der Pipeline aus pipeline.py. Werden hier grundlegende Änderungen vorgenomme, müsste model.py ebenfalls angepasst werden. Trotzdem hat auch die monolitische Architektur wichtige Vorteil gegenüber einem modularen Aufbau bzw. Microservices. Durch eine monolitische Architektur ist es möglich die Software leichter zu testen, als auch zu debuggen. . Dokumentation des Systems . Die Dokumentation wurde mithilfe von Pdoc erstellt. Dabei werden sämtliche Docstring, welche in den verschiedenen Skripten verwendet wurden, zu einer HTML-Datei zusammengefügt und über GitLab Pages bereitgestellt. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/05architektur.html",
            "relUrl": "/markdown/2022/06/24/05architektur.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Web - Technologien",
            "content": "Web-Technologien von Frunch Infinity . . Streamlit . Die Ergebnisse des Projekts bzw. des Auto-ML-Prozesses werden innerhalb einer Streamlit Applikation visualisiert. Diese wird wiederrum von Streamlit Cloud gehostet und ist somit öffentlich zugänglich. Durch Streamlit ist es einerseits sehr schnell möglich Ergebnisse anschaulich dazustellen, aber auch sie in einem relativ leichten Prozess durch GitHub und Streamlit Cloud zu deployen. . SQL-Alchemy (Data Warehouse) . Durch SQL-Alchemy stellen wir unserem System ein Data Warehouse bereit, in dem einerseits die Trainingsergebnisse unseres Modells abgespeichert und dementsprechend auch abgefragt werden können. Zudem wird das Target Feature, welches in der Streamlit-Applikation auswählbar ist, ebenfalls im Data Warehouse abgelegt. . PyCaret . Durch die Hilfe von PyCaret werden insgesamt 17 verschiedene Modelle trainiert und im Anschluss mit Hilfe verschiedener Metriken verglichen. Der ausgegebene Data Frame wir nach Beendigung des Trainings in der Streamlit App dargestellt . Poetry . Innerhalb der Projektorganisation nutzen das Package Management System von Poetry. Innerhalb des poetry.lock files werden sämtliche Paket Requirements gelistet. Durch Poetry arbeiten alle Entwickler auf den gleichen Versionsebenen der verschiedenen Pakete. . Hydra . Als Pfad-Management-Tool verwenden wir das Framework von Hydra. Durch Hydra ist es möglich beispielsweise den Daten-Pfad einmal zentral in config (main.yaml) festzulegen und diesen Ort durch den decorator von hydra überall anzusprechen. Sollte sich der Ort irgendwann ändern, kann dies zentral an einem Ort erfolgen . Cookiecutter . Für die Organisation und die Strukturierung des Projekt Repositorys haben ein Template von Cookiecutter verwendet. Auch wenn nicht alle Vorlagen des Templates verwendet wurden, hat dies bei der Übersichtlichkeit des Projekts geholfen . Comit Control . Durch die .pre-commit-config-yaml werden in einige Richtlinien für den Abschluss eines Commits festgelegt. Ein Beispiel für eine solche Richtlinie ist beispielsweise isort zu nennen. Hierbei werden die Imports automatisch in sinnvoller Reihenfolge und alphabetisch sortiert. . Docker Container . Docker Container werden innerhalb des Projektes einerseits für dev-dependencies, die Entwicklung in virtuellen Umgebungen und den Aufbau des Frontends verwendet . Logging . In einem .log file werden sämtliche Prozesse die in der Ausgabe beispielsweise beim Modell-Training oder im Preprocessing mitlaufen abgespeichert. Dies ist insbesondere für Debugging essentiell. . Unittest . Es gibt verschiedene Unit-Tests im Projekt. Das Framework testet dabei unter anderem innerhalb der Preprocessing-Pipeline auf die korrekte shape der Daten, ob die entsprechenden Variablen entfernt wurden und der Pfad stimmt. Zudem gibt es einen Test, ob die Streamlit-Applikation aufrufbar ist. Durch die Unit- tests wird das gesamte System robuster und vereinfacht darüber hinaus das Debugging. . Pdoc . Durch die Software von Pdoc ist es innerhalb des Projekts möglich, sämtliche Docstrings in einer html-Datei zusammenzufassen und dementsprechend wichtige Informationen zu dokumentieren. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/04web-technologien.html",
            "relUrl": "/markdown/2022/06/24/04web-technologien.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Data Science Methodik-Konzept - Beschreibung",
            "content": ". Abbildung des Datenflusses . . . Business Understanding . Das Ziel des Projektes ist die Entwicklung eines Auto Machine Learning Prozesses welcher innerhalb einer UI Schnittstelle ausgelöst werden kann. Ausgehend von diesem Ziel müssen die Rohdaten des Turbofan-Datensatzes innerhalb einer Pipeline für das anschließende Training mit Pycaret aufbereitet werden. Sobald das Training der Modelle beendet ist, muss der Output (Metriken) der Modelle übersichtlich in einem Dataframe dargestellt werden. Innerhalb einer Streamlit-Applikation muss es möglich sein, jenen Dataframe auszugeben, sowie ein “Retraining” der Pipeline erneut zu initiieren. . Data Understanding . Die Datenbasis bildet der Turbofan-Datensatz, welcher in mehreren txt Dateien in data/raw des Projekt-Repositorys abliegt. Die Datensätze bestehen dabei aus mehreren multivariaten Zeitreihen. Jeder Datensatz wird zudem in Trainings- und Test-Teilsätze unterteilt. Außerdem stammt jede Zeitreihe aus einer unterschiedlichen Maschine. Die Textdateien werden mit 26 numerischen Variablen, getrennt durch Leerzeichen, bereitgestellt. . Data Preparation . In der Datenaufbereitung wird der Trainingsdatensatz zunächst für die “Preprocessing Pipeline” aufbereitet. Wichtige Schritte sind hierbei die Bestimmung der Input Feature, sowie des Target Features. Während das Taret Feature die Sensor-Werte 1-23 annehmen kann, müssen die Variablen: Unit-Number, Cycles und Operational Settings 1-3 in kategoriale Variablen konvertiert werden. Im Anschluss werden mithilfe einer Pipeline, fehlende und None Werte entfernt. zudem werden die numerischen Variablen skaliert. . Modeling . Durch die PyCaret Funktion .setup werden die numerischen, kategorialen Variablen, sowie das Target Feature für das anschließende Training initialisiert. Das eigentliche Training erfolgt durch die Funktion .compare_models() . Evaluierung . Nachdem das Modell-Training beendet ist, wird durch die .pull() Funktion ein Data Frame erzeugt, welcher die Trainingsresultate zusammenfasst. Der Data Frame beinhaltet eine Auflistung der unterschiedlichen Modelle, dem entsprechenden MAE, MSE, RMSE, R^2, RMSLE, MAPE und die verstrichene Trainingstzeit. Anhand dieser Metriken kann jedes Modell evaluiert werden . Deployment . Die Applikation wird mit Streamlit Cloud deployed, da mithilfe dieses Dienstes der Veröffentlichungsprozess recht einfach umsetzbar ist. Um die Applikation public zu machen, wird ledigleich ein GitHub-Repository mit den selben Inhalten wie in GitLab Repository benötigt. Im Anschluss kann direkt über die lokale Streamlit-App die build-in Funktion “deploy app” verwendet werden. Nachdem man dort den entsprechenden GitHub-Account und Repository angegeben hat, wird die Applikation automatisch deployed. Hierbei kann es zu Problemen kommen, wenn die App die von Streamlit breitgestellten Ressourcen übertrifft. Dieses Problem kann innerhalb von Streamlit mit der cache()-Funktion (decorator) in der entsprechenden ui.py gelöst werden. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/02methodik-konzept.html",
            "relUrl": "/markdown/2022/06/24/02methodik-konzept.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Modellierte Anforderungen (Requirements)",
            "content": "Modellierte Requirements von Frunch Infinity . . . Veränderung der Requirements im Projektverlauf . In Stage 1 unseres Projektes haben wir die Requirements nach den Sophisten Regeln grundlegend festgelegt und auf Basis jener wurden die ersten, erforderlichen Entwicklungsschritte durchgeführt: . erstellung eines dev-containers | Aufbau einer preprocessing Pipeline | Aufbau einer CI/CD Pipeline | Unit-Tests für Pipelines | erste Version der Streamlit Applikation | Modellentwicklung mit PyCaret | . Nachdem die ersten Entwicklungsschritte von Stage 1 abgeschlossen waren, haben wir in unserem Sprint-Review einen neuen Requirement-Zyklus eingeleitet und die einzelnen Requirements neu priorisiert. Zunächst hat sich gezeigt das das Requirement: “Wenn der Turbofan-Datensatz in die Streamlit-Applikation importiert wurde, muss die Applikation alle Variablen und Werte des Datensatzes erkennen” nach unseren ersten Entwicklungserkenntnissen nicht ganz passend für unsere MVP-Software einer Auto-ML ist. Der Import von einem oder verschiedenen Datensätzen erschwert die Entwicklung eines ersten, funktionierenden Produkts enorm. Die Wichtigkeit unserer Requirements wurde also in diesem Zyklus angepasst auf zunächst einen Datensatz (Turbofan) und Regressions-Algorithmen priorisiert. . Nach jedem größeren Entwicklungsschritt bzw. in jedem Sprint-Review sollten die aufgestellten Requirements auf konsistenz und Vollständigkeit geprüft werden, da auf Veränderungen innerhalb der Requirements nur angemessen reagiert werden kann, wenn diese bekannt sind. .",
            "url": "https://n1klas96.github.io/SE_2_Portfolio/markdown/2022/06/24/01Requirements-Engineering.html",
            "relUrl": "/markdown/2022/06/24/01Requirements-Engineering.html",
            "date": " • Jun 24, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://n1klas96.github.io/SE_2_Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://n1klas96.github.io/SE_2_Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}